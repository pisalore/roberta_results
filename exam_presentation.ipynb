{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<p style=\"text-align:right; font-size:14px;\"> University of Studies of Florence\n",
    "<p style=\"text-align:right; font-size:14px;\"> Department of Engineering Information </p>\n",
    "<p style=\"text-align:right; font-size:14px;\"> Firenze, December 2021 </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 align=center>Machine Learning Exam</h2>\n",
    "<h3 align=center>RoBERTa by Facebook AI: some results reproduction.</h3>\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "__AUTHOR__ = {'lp': (\"Lorenzo Pisaneschi\",\n",
    "                     \"lorenzo.pisaneschi1@stud.unifi.it\",\n",
    "                     \"https://github.com/pisalore/roberta_results\")}\n",
    "\n",
    "__TOPICS__ = ['Natural Language Processing', 'GLUE Benchmark', 'RACE Dataset']\n",
    "\n",
    "__KEYWORDS__ = ['Machine Learning', 'AI', 'NLP', 'Transformers', 'BERT', 'fairseq', 'Facebook AI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "<br>\n",
    "Natural Language Processing (NLP) is a research field which interest linguistics, computer science and AI. It concerns\n",
    "how computers process, analyze and understand large amount of natural language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "NLP can be used for :\n",
    "<ul>\n",
    "    <li>Machine Translation</li>\n",
    "    <li>Sentiment Analysis</li>\n",
    "    <li>Linguistic acceptability</li>\n",
    "    <li>Sentences' similarity and entailment</li>\n",
    "    <li>Automatic QA / Virtual assistants</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Background</h2>\n",
    "\n",
    "NLP has its roots in 1950, when Alan Turing published **\"Computing Machinery and Intelligence\"** in *Mind*,\n",
    "an academic journal by Oxford Press. In this paper, Turing proposed the famous **Imitation game**, considering the question\n",
    "*\"Can machines think?\"*, opening technical and philosophical debates.\n",
    "\n",
    "From a technical point of view we can divide NLP in three major historical periods and methods:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Symbolic NLP (1950s – early 1990s)\n",
    "\n",
    "Given a **collection of rules**, the computer emulates language understanding,\n",
    "without applying any kind of learning. This is a deterministic approach. A related interesting work is philosopher John\n",
    "Searle's *\"Chinese room experiment\"*, where the **Weak AI** and **Strong AI** theories are proposed.\n",
    "\n",
    "The focus was on encoding natural languages (with semantics, morphology and syntax) in data for machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Statistical NLP (1990s–2010s)\n",
    "\n",
    "The gradual lessening of Chomsky's linguistics theories and the steady increase\n",
    "of computational power allowed using machine learning algorithms also in NLP. **IBM Research** implemented NLP\n",
    "statistical method focusing on machine translation, exploiting textual corpora.\n",
    "\n",
    "Then, in early 2000s, thanks to web, tons of unannotated data were provided, inducing researchers to focus on\n",
    "unsupervised learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Neural NLP (present)\n",
    "\n",
    "The use of deep learning, representation learning and word embeddings techniques made possible to achieve the\n",
    "state-of-the-art in many NLP tasks. In particular, neural machine translation attempts to use a single neural network,\n",
    "to read an input and translate the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural NLP: Methods & techniques\n",
    "\n",
    "As for each machine learning problems, three elements are the most important:\n",
    "1. **Data**: how they must be mapped to be conveniently processed by a neural network.\n",
    "\n",
    "2. **Architectures**: how they must be designed in such a way to let the learning process as generalizing as possible, fast and\n",
    "reliable.\n",
    "3.\n",
    "4. **Tasks**: the problems to be addressed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Neural Machine Translation\n",
    "\n",
    "A first important example to be considered is machine translation, event because it has been one of the first problem\n",
    "considered in NLP:\n",
    "\n",
    "<center> $ argmax_y p(y | x)$ </center>\n",
    "\n",
    "Find a target sentence **y** that maximizes the conditional probability of y given a source sentence **x**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Neural machine translation** aims at building a single neural network that can be jointly tuned to maximize the\n",
    "translation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Encoder-decoders** architectures are often used for the goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- However, problems raise when trying to increase the difficulty of the problem, for example, **fixed-length vectors** usage\n",
    "for long source sentences encoding of the most important information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bidirectional Recurrent Neural Network\n",
    "\n",
    "In *\"Neural machine translation by jointly learning to align and translate\"* (2016), Bahdanau et al. propose e method\n",
    "for which a model tries to translate and align a word using the contextual words vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Encoding the input sentence into a **sequence of vectors** and choose a subset of these vectors\n",
    "adaptively while decoding the translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Using a **Bidirectional RNN** for encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Using a **decoder** that emulates searching through a source sentence during translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bidirectional Recurrent Neural Network: some formula\n",
    "\n",
    "Given a sequence of vectors $ x = (x_1, ..., x_2)$, an encoder encodes it in a vector *c*:\n",
    "<center> $h_t = f(x_t, h_{t-1})$ </center>\n",
    "<center> $c = q({h_1, h_{Tx}})$ </center>\n",
    "Where $h_t \\in R^n$, $f$ and $q$ some non-linear functions, and $c$ is a vector generated from the hidden states.\n",
    "<br>\n",
    "The proposed idea uses a context vector $c_i$ for each target word $y_i$ s.t.:\n",
    "<center> $ p(y) = \\prod_{i=1}^{T} p(y_i | {y_1, ..., y_{i-1}, x})$ </center>\n",
    "\n",
    "$c_i$ depends on a **sequence of annotations**, where each annotation contains information about the whole input sequence,\n",
    "giving more importance to surroundings parts of *i-th* word.\n",
    "\n",
    "$c_i$ is a **weighted sum of annotations**; weights depend on an *alignment model* which measures how well the translated\n",
    "word and near input words match.\n",
    "\n",
    "This implements an **attention mechanism**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformers: \"Attention Is All You Need\"\n",
    "An **attention mechanism** enables to dynamically highlight relevant features of the input data, in NLP textual elements,\n",
    "obviously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In *\"Attention Is All You Need* (2017), Vaswani et al. proposed the famous **Transformer** architecture, which relies\n",
    "only on attention mechanism, specifically **self-attention**, without recurrence, in order to:\n",
    "\n",
    "- Improve performance\n",
    "- Let tasks parallelization possible\n",
    "- Improve long sentence handling\n",
    "- Improve generalization on other tasks, not only transduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transformer: the model\n",
    "\n",
    "<div>\n",
    "    <div style=\"float: right\">\n",
    "        <img src=\"images/transformer_architecture.png\" width=\"480\">\n",
    "    </div>\n",
    "    <br>\n",
    "     <div>\n",
    "        <li>The <strong>transformer</strong> is a <strong>encoder-decoder</strong> architecture. Each step is autoregressive: outputs are used as additional input generating the next.</li>\n",
    "        <br>\n",
    "        <li>The <strong>encoder</strong> maps a sequence of symbols $x = (x_1, x_2..., x_n)$ to a sequence of continuous representation $z = (z_1, z_2, ..., z_n)$</li>\n",
    "        <br>\n",
    "        <li>The <strong>decoder</strong> then generates an output $y = (y_1, y_2, ..., y_n)$ one element at time.</li>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transformer: the attention mechanism\n",
    "\n",
    " <img src=\"images/transformer_attention.png\">\n",
    "\n",
    " <div>\n",
    "    <li><strong>Scaled Dot-Product Attention</strong>: $Attention(Q, K, V) = softmax(\\frac{QK^T} {\\sqrt{d_k}})V$</li>\n",
    "    <li><strong>Multi-Head Attention</strong>: $MultiHead(Q, K, V) = Concat(head_1,...head_h)W^O$\n",
    "    <br>\n",
    "    $head_i = Attention(QW_i^Q, QW_i^K, KW_i^K, VW_i^V)$</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transformer: summary\n",
    "\n",
    "- A Transformer is a deep learning model that adopts **attention** mechanisms which weighs the importance\n",
    "of different parts of input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Data are not processed in order, as in RNN or LSTM: the transformer tries to give context to words in sentences. This\n",
    "key idea allows parallelization and improves performance, generalizing for a wide range of tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Transformer is an **encoder-decoder** architecture. The encoder encodes information about importance of parts with\n",
    "respect to others in sentence in word encodings. The first encoder  layer simply takes **words embeddings**.\n",
    "On the other hand, the decoder generates an output sequence of probabilities over the vocabulary, starting from encodings\n",
    "shifted by one position\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The model learns three matrices for each attention unit $W_Q, W_K, W_V$. The **training** is usually done in a\n",
    "semi-supervised fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BERT: Bidirectional Encoder Representations from Transformers by Google AI\n",
    "\n",
    "Transformers revolutionized NLP, and research worked in this direction. With *\"BERT: Pre-training of Deep Bidirectional\n",
    "Transformers for Language Understanding* (2018) Devlin et al. proposed a new model to pre-train deep\n",
    "bidirectional representations from unlabelled text; as a result, the pre-trained BERT can be fine-tuned to obtain\n",
    "new state-of-the-art results in many tasks.\n",
    "\n",
    "BERT rapidly became an important baseline for NLP research and experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## BERT architecture\n",
    "\n",
    "<img src=\"images/bert.png\">\n",
    "\n",
    "- Exploit **bidirectional pre-training** for language representations\n",
    "- Exploit **fine-tuning** and **transfer learning** in order to avoid the use of heavy engineered architectures\n",
    "- Pre-training is done with **unlabelled data**, while fine-tuning works with labelled data for the specific task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## BERT input representation and pre-training\n",
    "<img src=\"images/bert2.png\">\n",
    "\n",
    "- To work with different **downstream tasks** input are represented in order to disambiguate both single sentence and\n",
    "sentences pairs. **WordPiece** embeddings is used.\n",
    "- As training data, the **BooksCorpus** (800M words) is used, along with **English Wikipedia (2500 words)\n",
    "- For bidirectional training, some percentage of input tokens are masked (MLM); however it creates a downside, since fine-tuning\n",
    "procedure does not use [MASK] tokens. Next Sentence Prediction (NSP) is used, in order to train a model to understand\n",
    "sentences relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fine-tuning and experiments\n",
    "\n",
    "- Transformer **self-attention** allows to model many downstream tasks involving single text or text pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For fine-tuning, we simply plug in the task-specific inputs and outputs into BERT. Experiments involved:\n",
    "- Sentences equivalence\n",
    "- Sentences entailment\n",
    "- Question Answering\n",
    "- Text classification (Linguistic Acceptability)\n",
    "\n",
    "The metrics depend on the interested benchmarks and datasets.\n",
    "\n",
    "Two BERT models have been used: $BERT_{base}$ (L=12, H=768, A=12 for 110M parameters) and $BERT_{large}$\n",
    "(L=24, H=1024, A=16 for 340M parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RoBERTa by Facebook AI\n",
    "\n",
    "As mentioned before, BERT became an essential baseline for NLP research.\n",
    "\n",
    "Among other models, Facebook AI presented RoBERTa in **fairseq**, a sequence modeling toolkit written in PyTorch.\n",
    "\n",
    "In *\"RoBERTa: A Robustly Optimized BERT Pretraining Approach* (2019) Liu et al. proposed an **optimized version of BERT**,\n",
    "considering the following key points:\n",
    "\n",
    "- Training the model **longer**, with bigger batches using **more data**\n",
    "- Removing the use of **NSP** (Next Sentence Prediction) loss\n",
    "- Training on **longer sequences**\n",
    "- Using **Dynamic Masking** on training data: masking is done during training, not once during data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## RoBERTa: some results reproduction\n",
    "\n",
    "I have experiment with two different benchmarks for investigate **RoBERTa** power:\n",
    "- **GLUE benchmark** (General Language Understanding Evaluation)\n",
    "- **RACE benchmark** (Large-scale ReAding Comprehension Dataset From Examination)\n",
    "\n",
    "For this purpose, I have worked at different levels:\n",
    "\n",
    "1. **Data analysis**: understanding data shape\n",
    "2. **Data preprocessing**: using BPE (Byte Pair Encoding)\n",
    "3. **Pretrained model fine-tuning**: using the indicated hyperparameters\n",
    "4. **Test inference**: on development sets of each dataset using different metrics\n",
    "\n",
    "It is important to stress that RoBERTa models have been pre-trained:\n",
    "\n",
    "1. with 160GB of uncompressed text (**BookCorpus + English Wikipedia**, **CC-News**, **Stories**)\n",
    "2. for longer, 500k epochs\n",
    "\n",
    "**Experiments were conducted on a `NVIDIA GeForce RTX 3090` GPU, 24GB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## GLUE benchmark\n",
    "\n",
    "GLUE benchmark consists of 9 different datasets for as much NLP tasks.\n",
    "\n",
    "Online at https://gluebenchmark.com/ a leaderboard is present.\n",
    "\n",
    "**RoBERTa** model achieved the bests results on 4/9 of the GLUE tasks when was published: **MNLI, QNLI, RTE and STS-B**\n",
    "\n",
    "In the next I list all the results' reproduction I made, starting from the fine-tuning of RoBERTa base and large models\n",
    "(provided by Facebook AI) to results validation on development datasets of various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MNLI (The Multi-Genre Natural Language Inference Corpus)\n",
    "\n",
    "*[...] is a crowdsourced collection of sentence pairs with textual entailment annotations. Given a premise sentence\n",
    "and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts\n",
    "the hypothesis (contradiction), or neither (neutral).*\n",
    "\n",
    "**Matched** sentences belong to same domain, the other way round for **mismatched** sentences\n",
    "\n",
    "- **392703** annotated train sentences\n",
    "- **9816** annotated matched development sentences\n",
    "- **9833** annotated mismatched development sentences\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "| **num classes** | **learning rate** | **max sentences** | **total num updates** | **warmup updates** |\n",
    "|-------------|---------------|---------------|-------------------|----------------|\n",
    "| 3           | 1.0e-05       | 32            | 123873            | 7432           |\n",
    "\n",
    "\n",
    "**Results**\n",
    "\n",
    "| model                \t| Accuracy (mismatched) \t| Accuracy (matched) \t|\n",
    "|----------------------\t|-----------------------\t|--------------------\t|\n",
    "| roberta.base      \t| 86,7%                 \t| 87,2%              \t|\n",
    "| roberta.large.mnli \t| 90,1%                 \t| 90,59%             \t|\n",
    "\n",
    "<style>\n",
    "td {\n",
    "  font-size: 18px\n",
    "}\n",
    "th {\n",
    "  font-size: 14px\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### QNLI (Stanford Question Answering Dataset)\n",
    "\n",
    "*[...] The Stanford Question Answering Dataset (Rajpurkar et al. 2016) is a question-answering\n",
    "dataset consisting of question-paragraph pairs, where one of the sentences in the paragraph (drawn\n",
    "from Wikipedia) contains the answer to the corresponding question (written by an annotator).*\n",
    "\n",
    "- **104744** annotated train sentences\n",
    "- **5464** annotated development sentences\n",
    "\n",
    "Example: *17\tWhat percentage of farmland grows wheat?\tMore than 50% of this area is sown for wheat, 33% for barley and 7% for oats.\tentailment*\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "| **num classes** | **learning rate** | **max sentences** | **total num updates** | **warmup updates** |\n",
    "|-------------|---------------|---------------|-------------------|----------------|\n",
    "| 2           | 1.0e-05       | 32            | 123873            | 7432           |\n",
    "\n",
    "\n",
    "**Results**\n",
    "\n",
    "| model                \t| Accuracy  \t|\n",
    "|----------------------\t|-----------------------\t|\n",
    "| roberta.base      \t| 93,2%                 \t|\n",
    "| roberta.large.mnli \t| 94,4%                 \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### QQP (Quora Question Pairs)\n",
    "\n",
    "*[...] The Quora Question Pairs2 dataset is a collection of question pairs from the community\n",
    "question-answering website Quora. The task is to determine whether a pair of questions are semantically equivalent.*\n",
    "\n",
    "- **363847** annotated train sentences\n",
    "- **40431** annotated development sentences\n",
    "\n",
    "Example: *201359\t303345\t303346\tWhy are African-Americans so beautiful?\tWhy are hispanics so beautiful?\t0*\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "| **num classes** | **learning rate** | **max sentences** | **total num updates** | **warmup updates** |\n",
    "|-------------|---------------|---------------|-------------------|----------------|\n",
    "| 2           | 1.0e-05       | 32            | 123873            | 7432           |\n",
    "\n",
    "\n",
    "**Results**\n",
    "\n",
    "| model                \t| Accuracy  \t|\n",
    "|----------------------\t|-----------------------\t|\n",
    "| roberta.base      \t| 93,2%                 \t|\n",
    "| roberta.large.mnli \t| 94,4%                 \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RTE (Recognizing Textual Entailment)\n",
    "\n",
    "*The Recognizing Textual Entailment (RTE) datasets come from a series of annual textual\n",
    "entailment challenges. All datasets are converted to a two-class split, where\n",
    "for three-class datasets we collapse neutral and contradiction into not entailment, for consistency.*\n",
    "\n",
    "As it is indicated in the official paper, the large model for finetuning on RTE task is the one obtained from a\n",
    "pre-finetuning of `roberta.large` on MNLI dataset.\n",
    "\n",
    "- **2491** annotated train sentences\n",
    "- **278** annotated development sentences\n",
    "\n",
    "Example: *0\tNo Weapons of Mass Destruction Found in Iraq Yet.\tWeapons of Mass Destruction Found in Iraq.\tnot_entailment*\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "| **num classes** | **learning rate** | **max sentences** | **total num updates** | **warmup updates** |\n",
    "|-------------|---------------|---------------|-------------------|----------------|\n",
    "| 2           | 2.0e-05       | 16            | 2036            | 122           |\n",
    "\n",
    "\n",
    "**Results**\n",
    "\n",
    "| model                \t| Accuracy  \t|\n",
    "|----------------------\t|-----------------------\t|\n",
    "| roberta.base      \t| 79,0%                 \t|\n",
    "| roberta.large.mnli \t| 90,1%                 \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SST-2 (Stanford Sentiment Treebank)\n",
    "\n",
    "*The Stanford Sentiment Treebank consists of sentences from movie\n",
    "reviews and human annotations of their sentiment. The task is to predict the sentiment of a given\n",
    "sentence. Labels are from two-way (positive/negative) class split.*\n",
    "\n",
    "As it is indicated in the official paper, the large model for finetuning on STS task is the one obtained from a\n",
    "pre-finetuning of `roberta.large` on MNLI dataset.\n",
    "\n",
    "- **67350** annotated train sentences\n",
    "- **873** annotated development sentences\n",
    "\n",
    "Example: *allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker. 1*\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "| **num classes** | **learning rate** | **max sentences** | **total num updates** | **warmup updates** |\n",
    "|-------------|---------------|---------------|-------------------|----------------|\n",
    "| 2           | 1.0e-05       | 32            | 20935            | 1256           |\n",
    "\n",
    "\n",
    "**Results**\n",
    "\n",
    "| model                \t| Accuracy  \t|\n",
    "|----------------------\t|-----------------------\t|\n",
    "| roberta.base      \t| 95,0%                 \t|\n",
    "| roberta.large.mnli \t| 96,0%                 \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MRPC (Microsoft Research Paraphrase Corpus)\n",
    "\n",
    "*The Microsoft Research Paraphrase Corpus (Dolan & Brockett, 2005) is a corpus of sentence pairs automatically\n",
    "extracted from online news sources, with human annotations for whether the sentences in the pair are semantically\n",
    "equivalent.*\n",
    "\n",
    "As it is indicated in the official paper, the large model for finetuning on STS task is the one obtained from a\n",
    "pre-finetuning of `roberta.large` on MNLI dataset.\n",
    "\n",
    "- **4077** annotated train sentences\n",
    "- **1726** annotated development sentences\n",
    "\n",
    "Example: *1\t702876\t702977\tAmrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .\tReferring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .*\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "| **num classes** | **learning rate** | **max sentences** | **total num updates** | **warmup updates** |\n",
    "|-------------|---------------|---------------|-------------------|----------------|\n",
    "| 2           | 1.0e-05       | 16            | 2296            | 137           |\n",
    "\n",
    "\n",
    "**Results**\n",
    "\n",
    "| model                \t| F1 score  \t|\n",
    "|----------------------\t|-----------------------\t|\n",
    "| roberta.base      \t| 90,0%                 \t|\n",
    "| roberta.large.mnli \t| 92,3%                 \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### CoLA  (Corpus of Linguistic Acceptability)\n",
    "\n",
    "*The Corpus of Linguistic Acceptability (Warstadt et al., 2018) consists of English acceptability judgments drawn from\n",
    "books and journal articles on linguistic theory*\n",
    "\n",
    "- **8551** annotated train sentences\n",
    "- **1043** annotated development sentences\n",
    "\n",
    "Example: *cj99\t1\t\tIf you had eaten more, you would want less.\n",
    "cj99\t0\t*\tAs you eat the most, you want the least.*\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "| **num classes** | **learning rate** | **max sentences** | **total num updates** | **warmup updates** |\n",
    "|-------------|---------------|---------------|-------------------|----------------|\n",
    "| 2           | 1.0e-05       | 16            | 5336            | 320           |\n",
    "\n",
    "\n",
    "**Results**\n",
    "\n",
    "| model                \t| Matthew's Corr  \t|\n",
    "|----------------------\t|-----------------------\t|\n",
    "| roberta.base      \t| 62,0%                 \t|\n",
    "| roberta.large.mnli \t| 68,7%                 \t|\n",
    "\n",
    "Considering the **Matthew's Correlation Coefficient**:\n",
    "\n",
    "$MCC = \\frac{TP * TN - FP * FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### STS-B  (Corpus of Linguistic Acceptability)\n",
    "\n",
    "*The Semantic Textual Similarity Benchmark (Cer et al., 2017) is a collection of sentence\n",
    "pairs drawn from news headlines, video and image captions, and natural language inference data.\n",
    "Each pair is human-annotated with a similarity score from 1 to 5; the task is to predict these scores.*\n",
    "\n",
    "- **5750** annotated train sentences\n",
    "- **1501** annotated matched development sentences\n",
    "\n",
    "Example: *1\tmain-captions\tMSRvid\t2012test\t0002\tnone\tnone\tA young child is riding a horse.\tA child is riding a horse.\t4.750*\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "| **num classes** | **learning rate** | **max sentences** | **total num updates** | **warmup updates** |\n",
    "|-------------|---------------|---------------|-------------------|----------------|\n",
    "| 1           | 2.0e-05       | 16            | 3598            | 214           |\n",
    "\n",
    "\n",
    "**Results**\n",
    "\n",
    "| model                \t| Pearson-Spearman Corr  \t|\n",
    "|----------------------\t|-----------------------\t|\n",
    "| roberta.base      \t| 91,0%                 \t|\n",
    "| roberta.large.mnli \t| 92,2%                 \t|\n",
    "\n",
    "Considering the **Pearson-Spearman Correlation Coefficient**:\n",
    "\n",
    "$\\rho_{XY} = \\frac {\\sigma_{XY}}{\\sigma_X\\sigma_Y}$\n",
    "where X and Y are development set labels and predictions, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## RACE benchmark\n",
    "\n",
    "RACE is a large-scale reading comprehension dataset. In consists of **passages**, **multiple choice questions**.\n",
    "This dataset is collected from English exams in China for middle and high school students.\n",
    "\n",
    "Online at http://www.qizhexie.com/data/RACE_leaderboard.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```json\n",
    "{\n",
    "   \"answers\":[\n",
    "      \"D\",\n",
    "      \"A\",\n",
    "      \"B\",\n",
    "      \"B\"\n",
    "   ],\n",
    "   \"options\":[\n",
    "      [\n",
    "         \"Help his mother.\",\n",
    "         \"Watch TV.\",\n",
    "         \"Wear his raincoat\",\n",
    "         \"Go out.\"\n",
    "      ],\n",
    "      [\n",
    "         \"happy\",\n",
    "         \"scary\",\n",
    "         \"dangerous\",\n",
    "         \"boring\"\n",
    "      ],\n",
    "      [\n",
    "         \"The raincoat can stop the rain.\",\n",
    "         \"The color of Robbie's raincoat is red.\",\n",
    "         \"Robbie first watches with his Mum\",\n",
    "         \"Robbie's mum doesn't wear a raincoat in the rain.\"\n",
    "      ],\n",
    "      [\n",
    "         \"It's raining\",\n",
    "         \"Fun in the rain\",\n",
    "         \"Robbie and His mother\",\n",
    "         \"Robbie's raincoat\"\n",
    "      ]\n",
    "   ],\n",
    "   \"questions\":[\n",
    "      \"What does Robbie want to do on the rainy day?\",\n",
    "      \"Robbie has a_day that day.\",\n",
    "      \"Which of the following is TRUE according to the passage?\",\n",
    "      \"Which is the best title for the passage?\"\n",
    "   ],\n",
    "   \"article\":\"Pit-a-pat. Pit-a-pat. It's raining. \\\"I want to go outside and play, Mum,\\\" Robbie says, \\\"When can the rain stop?\\\" His mum doesn't know what to say. She hopes the rain can stop, too. \\\"You can watch TV with me,\\\" she says. \\\"No, I just want to go outside.\\\" \\\"1Put on your raincoat.\\\" \\\"Does it stop raining?\\\" \\\"No, but you can go outside and play in the rain. Do you like that?\\\" \\\"Yes, mum.\\\" He runs to his bedroom and puts on his red raincoat. \\\"Here you go. Go outside and play.\\\" Mum opens the door and says. Robbie runs into the rain. Water goes 2here and there. Robbie's mum watches her son. He is having so much fun. \\\"Mum, come and play with me!\\\" Robbie calls. The door opens and his mum walks out. She is in her yellow raincoat. Mother and son are out in the rain for a long time. They play all kinds of games in the rain.\",\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \"id\":\"middle10.txt\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## RACE fine-tuning and evaluation\n",
    "\n",
    "Given a question, it is concatenated with each of its possible answers; then, these sequences are passed through a\n",
    "fully-connected layer to predict the correct answer.\n",
    "\n",
    "**Data**\n",
    "- 28,000 passages\n",
    "- 100,000 questions\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "| **num classes** | **learning rate** | **max sentences** | **update frequency**   |\n",
    "|-------------|---------------|---------------|-------------------|---------------- |\n",
    "| 4           | 1.0e-05       | 16            | 5336            |\n",
    "\n",
    "**Results**\n",
    "\n",
    "| model                \t| Accuracy (Middle school) \t| Accuracy (High school) \t|\n",
    "|----------------------\t|-----------------------\t|--------------------\t    |\n",
    "| roberta.large      \t| 81,8%                 \t| 87,7%              \t    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "**Code**\n",
    "- https://github.com/pisalore/roberta_results, Github repository for this work\n",
    "- https://github.com/pytorch/fairseq/tree/main/examples/roberta, Fairseq RoBERTa\n",
    "\n",
    "**Papers**\n",
    "- https://arxiv.org/abs/1409.0473 Badhanau et al., Neural Machine Translation by Jointly Learning to Align and Translate\n",
    "- https://arxiv.org/abs/1706.03762 Vaswani et al., \"Attention Is All You Need\"\n",
    "- https://arxiv.org/abs/1810.04805, Devlit et al., \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
    "- https://arxiv.org/abs/1907.11692, Liu et al., \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\"\n",
    "\n",
    "**Dataset**\n",
    "- https://www.cs.cmu.edu/~glai1/data/race/, RACE dataset\n",
    "- https://gluebenchmark.com/, GLUE"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "PyCharm (roberta_results)",
   "language": "python",
   "name": "pycharm-7ff0cd8a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}